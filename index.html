<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<!-- original template from from url=(0035)http://www.cs.berkeley.edu/~barron/ -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
	<meta name="viewport" content="width=800">
    <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
     <style type="text/css">
    /* Color scheme stolen from Sergey Karayev */
    a {
    color: #1772d0;
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px
    }
    strong {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    }
    heading {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 22px;
    }
    papertitle {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 14px;
    font-weight: 700
    }
    name {
    font-family: 'Lato', Verdana, Helvetica, sans-serif;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 160px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
	
    <link href='http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link rel="icon" type="image/png" href="http://www.cs.berkeley.edu/~barron/seal_icon.png">
	
    <title>Dr. Kuldeep Purohit</title>
    
    <link href="/img/css" rel="stylesheet" type="text/css">
  </head>
  <body>
    <table width="1000" border="0" align="center" cellspacing="0" cellpadding="0">
      <tbody><tr>
        <td>
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td width="68%" valign="middle">
                <p align="center">
                  <name>Kuldeep Purohit</name>
                  
                </p><p align="">I am a Post-doctoral Research Associate at the <a href="http://www.cse.msu.edu/">Computer Science and Engineering Department at Michigan State University</a>. I work with  <a href="http://hal.cse.msu.edu/team/vishnu-boddeti/">Prof. Vishnu Boddeti</a> on topics related to Computer Vision and Image Processing . I finished my PhD at <a href="http://www.ee.iitm.ac.in/ipcvlab/">Image Processing and Computer Vision Lab</a>, Indian Institute of Technology Madras, Chennai under the supervision of <a href="http://www.ee.iitm.ac.in/~raju/">Prof. A. N. Rajagopalan</a>. I received my Bachelors Degree in Electrical Engineering from <a href="http://www.iitmandi.ac.in/">Indian Institute of Technology Mandi, Himachal Pradesh</a>. My research lies at the intersection of Image Processing/Computer Vision and Deep Learning. My recent works have focused on restoration of images and videos suffering from blur, low-resolution, rain, and haze and their utilization for scene segmentation and estimation of 3D geometry and motion.
</br>
		  </br>
	<!--	      Before that, I was a postdoctoral research fellow in <a href="http://disi.unitn.it/~mhug/index.html">Deep Relational Learning</a> group at the University of Trento with Professor <a href="http://disi.unitn.it/~sebe/">Nicu Sebe</a> and a visiting researcher in the CS department at the <a href="https://www.cs.washington.edu/">University of Washington</a> working with <a href="http://homes.cs.washington.edu/~ali/">Ali Farhadi</a>.
I did my PhD at the <a href="http://www.iit.it/">Italian Institute of Technology</a> where I was advised by Professor <a href="http://profs.sci.univr.it/~swan/">Vittorio Murino</a> and also working closely with Professor <a href="http://www0.cs.ucl.ac.uk/staff/m.pontil/">Massimiliano Pontil</a> from University College London.
	    </br></br>
		  I am so delighted to start computer vision research with Professor <a href="https://explorecourses.stanford.edu/instructor/mehrdads">Mehrdad Shahshahani</a> at <a href="http://www.ipm.ac.ir/">IPM Vision Group</a>.
I did my masters in AI and my bachelors in software engineering in Iran.
	  </br>
-->
<!--I had the opportunity to work under
I started Computer Vision with Mehrdad Shahshahani
Prior to my Ph.D., I spent one year as a research assistant at MI&V lab in <a href="http://www.test.com">Sharif University of Technology</a>. I was also fortunate enough to be advised by Professor <a href="http://www.test.com">Mehrdad Shahshahani</a> at the <a href="http://www.test.com">IPM Vision Group</a> from 2009 until 2011. I used to collaborate with IPPR lab at <a href="http://www.test.com">Amirkabir University of Technology</a> under supervision of Professor <a href="http://www.test.com">Mohammad Rahmati</a> as well.
I received my Master Degree on Artificial Intelligence from Tehran Polytechnic and my Bachelor Degree on Software Engineering from Shomal University at Amol (my home town). -->
                </p><p align="center">
<!--<a href="ee14s007@ee.iitm.ac.in">Email</a> &nbsp;/&nbsp;
<!--<a href="./files/cv.pdf">CV</a> &nbsp;/&nbsp; -->
<!--<a href="https://scholar.google.it/citations?user=31seHAMAAAAJ&hl=en">Google Scholar</a> &nbsp;/&nbsp; -->
<!--<a href="./files/Thesis_compressed.pdf">Thesis</a> &nbsp;/&nbsp; -->
<!--<a href="https://scholar.google.com/citations?hl=en&user=31seHAMAAAAJ&view_op=list_works&sortby=pubdate">Google Scholar</a> &nbsp;/&nbsp; -->
<a href="https://scholar.google.co.in/citations?user=QSvpXjYAAAAJ&hl=en">Google Scholar</a> /&nbsp;
<a href="https://www.linkedin.com/in/kuldeeppurohit3/"> LinkedIn </a> /&nbsp;
<a href="./files/CV_KuldeepPurohit2.pdf"> CV </a>
                </p>
              </td>
              <!--<td width="33%"><img src="./img/moin_pic_cool.jpg"></td>-->
				<td> <img src="./img/IMG-20191112-WA0020.jpg" style="width: 200;"></td></tr> 
          </tbody></table>
 <!--         <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Research</heading>
                <p> I work primarily on computer vision, but I am also interested in machine learning and pattern recognition. The central goal of my research is to use vast amounts of data to understand the underlying semantics and structure of visual contents. I am especially interested in learning and recognizing visual object categories and understanding human behaviors. I spent my Ph.D. working on learning mid-level representations for visual recognition (image and video understanding) and now, I am more focused on learning deep neural networks from noisy and incomplete multi-modal data.</p>-->
              <!--<p>My research lies at the intersection of machine learning, computer vision, and natural language processing with an emphasis on learning Deep Neural Networks with minimal supervision and noisy/incomplete multi-modal data.
		      </br></br>
		      <span class="highlight"><strong>Internship Position: </strong> I like working with students. If you're a PhD student interested in a research internship working with me in Berlin, please send me an email with your CV and research interests.</span>
		      </p>
		    </td>
            </tr>
          </tbody></table>
-->
<!--SECTION -->

<!--          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>News</heading>
		      
		      
		<p> <strong>[2017.09.19]</strong> Our <a href="https://arxiv.org/abs/1708.09644">paper</a> is awarded the <strong><a href="http://2017.ieeeicip.org/AwardFinalist.asp">Best Student Paper</a></strong> in ICIP 2017. Congrats Mahdyar!</p>
                <p> <strong>[2017.05.15]</strong> I will join <strong><a href="https://icn.sap.com/home.html">SAP Machine Learning Research</a></strong> in Berlin, as a <strong>Senior Research Scientist</strong> in Deep Learning.</p>
		<p> <strong>[2017.04.01]</strong> Two papers are accepted in <strong><a href="http://acl2017.org/">ACL 2017</a>.</strong> Congrats Azad and Ravi!</p>
                <p> <strong>[2016.12.01]</strong> I will present <strong><a href="https://arxiv.org/pdf/1611.06764.pdf">Plug-and-Play Binary Quantization Layer</a></strong> at Workshop on Efficient Deep Learning at NIPS 2016!</p>
		<p> <strong>[2016.11.11]</strong> The <strong><a href="https://github.com/hosseinm/med">Motion Emotion Dataset (MED)</a></strong> is online!</p>
                <p> <strong>[2016.09.25]</strong> Our work is finalist for the <strong><a href="http://2016.ieeeicip.org/Awards.asp">Best Paper Award</a></strong> in ICIP 2016.</p>

              </td>
            </tr>
          </tbody></table>
-->



          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>News</heading>
		<p> [2020.03] Our <a href="https://arxiv.org/abs/2004.05343">paper</a> has been accepted at <strong><a href="http://cvpr2020.thecvf.com/">CVPR 2020</a></strong>, Seattle, USA.</p>
		<p> [2020.01] I have been awarded <strong>ACM-India Travel Grant</strong> and <strong>AAAI Scholarship</strong> to present our <a href="https://arxiv.org/abs/1903.11394">paper</a> at <strong><a href="https://aaai.org/Conferences/AAAI-20/">AAAI 2020</a></strong>, New York, USA.</p>
		<p> [2019.12] Invited to be a reviewer for the IEEE Transactions on Image processing (TIP), IEEE Transactions on Multimedia (TMM) and International Journal of Computer Vision (IJCV) (Springer). </p>
                <p> [2019.12] Invited to present my research at the <strong><a href="http://ncvpripg.kletech.ac.in/cfv.html/">Vision India Track</a></strong> in the National Conference on Computer Vision, Pattern Recognition, Image Processing and Graphics (NCVPRIPG 2019) (Springer). </p>		      
		<p> [2019.11] Our team won the runner up prizes in <strong><a href="http://www.vision.ee.ethz.ch/aim19/">ICCV-AIM 2019</a></strong> Bokeh Effect Challenge and Realistic Image Super-resolution Challenge.</p>		     
		<p> [2019.05] I have been awarded <strong><a href="https://buildyourfuture.withgoogle.com/scholarships/google-travel-scholarships/#!?detail-content-tabby_activeEl=overview">Google Travel Grant</a></strong> to present our <a href="https://arxiv.org/abs/1804.02913">paper</a> at <strong><a href="http://cvpr2019.thecvf.com/">CVPR 2019</a></strong>, Long Beach, CA, USA.</p>
		<p> [2019.04] Our team has claimed <strong><a href="http://www.vision.ee.ethz.ch/ntire19/">1st position in CVPR-NTIRE 2019</a></strong> Image Colorization Challenge. We are also among the finalists in Video Deblurring, Video Superresolution, and Image Dehazing challenges of NTIRE 2019.</p>		      
		<p> [2018.12] Our <a href="./files/ICVGIP2018_Cam.pdf">work</a> is selected for the <strong><a href="https://cvit.iiit.ac.in/icvgip18/bestpaperaward.php">Best Paper Award (Runner Up)</a></strong> in ICVGIP 2018.</p>	
		<p> [2018.08] Our team is among the finalists in all the 3 tracks of <strong><a href="https://pirm2018.org/">ECCV-PIRM 2018</a></strong> Perceptual Image Super-resolution Challenge.</p>	
		<p> [2017.09] Our team secured 13th rank among 4000 participants in the <a href="https://www.hackerearth.com/challenges/competitive/deep-learning-challenge-1/leaderboard/">Hackerearth's Deep Learning based Object Classification Challenge</a>.</p>			      
		<p> [2017.01] Started working as a Research Intern at <a href="https://www.kla-tencor.com/">KLA-Tencor</a>, Chennai.</p>	
                <p> [2016.09] I will be presenting three papers from our lab at <strong><a href="http://www.2016.ieeeicip.org/ICIP%202016/www2.securecms.com/ICIP2016/default.html">ICIP 2016</a></strong>, Phoenix, AZ, USA.</p>		      
		<p> [2016.03] Started working on a sponsored Research Project with <a href="https://www.kla-tencor.com/">NIOT</a>, Ministry of Earth Sciences, Govt. of India.</p>			      

              </td>
            </tr>
          </tbody></table>

	  
	  
	  
	  <!--SECTION -->

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Publications and Preprints</heading>
              </td>
            </tr>
          </tbody></table>


          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>	  
	  
<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./files/ICCV_thumbprint.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Region-Adaptive Dense Network for Efficient Motion Debluring</papertitle></a><br><strong>Kuldeep Purohit</strong> and A.N. Rajagopalan<br>
                  <strong>AAAI 2020</strong> <br>
                  <a href="https://arxiv.org/abs/1903.11394">ArXiv Version</a> 
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a> -->
                </p><p></p>
                <p>In this paper, we address the problem of dynamic scene deblurring in the presence of motion blur. Restoration of images affected by severe blur necessitates a network design with a large receptive field, which existing networks attempt to achieve through simple increment in the number of generic convolution layers, kernel-size, or the scales at which the image is processed. However, these techniques ignore the non-uniform nature of blur, and they come at the expense of an increase in model size and inference time. We present a new architecture composed of region adaptive dense deformable modules that implicitly discover the spatially varying shifts responsible for non-uniform blur in the input image and learn to modulate the filters. This capability is complemented by a self-attentive module which captures non-local spatial relationships among the intermediate features and enhances the spatially-varying processing capability. We incorporate these modules into a densely connected encoder-decoder design which utilizes pre-trained Densenet filters to further improve the performance. Our network facilitates interpretable modeling of the spatially-varying deblurring process while dispensing with multi-scale processing and large filters entirely. Extensive comparisons with prior art on benchmark dynamic scene deblurring datasets clearly demonstrate the superiority of the proposed networks via significant improvements in accuracy and speed, enabling almost real-time deblurring.
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>
	  
	  
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>	  

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/PD_plot.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	        <!--        <p><a href="https://arxiv.org/abs/1605.07651"> -->
	<papertitle>Depth-guided Dense Dynamic Filtering Network for Bokeh Effect Rendering</papertitle></a><br><strong>Kuldeep Purohit</strong>*, Maitreya Suin, Praveen Kandula, and A.N. Rajagopalan<br>
			                  <strong>AIM Workshop and Challenge, International Conference on Computer Vision (ICCV 2019), Seoul, South Korea, November 2019</strong> <br>

	 <!-- 
 <a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Purohit_Scale-Recurrent_Multi-Residual_Dense_Network_for_Image_Super-Resolution_ECCVW_2018_paper.pdf">Paper</a> /
 <a href="./files/Poster_ECCV2018.pdf">Poster</a> 

               <a href="https://arxiv.org/abs/1804.02913.pdf">ArXiv Pre-print</a>
	  	  <a href="https://github.com/moinnabi/SelfPacedDeepLearning">code</a> /
                  <a href="http://dblp.uni-trier.de/rec/bib2/journals/corr/SanginetoNCS16.bib">bibtex</a> -->
                </p><p></p>
                <p>Our work presented at the International Conference on Computer Vision (ICCV) - Advances in Image Manipulation (AIM) Workshop 2019. Our team was a <strong>runner-up in both the tracks of Bokeh Effect Challenge</strong> (http://www.vision.ee.ethz.ch/aim19/).
</br></br> 
<!-- <small>*Authors contributed equally</small>-->

		</p><p></p>
                <p></p>
              </td>
            </tr>	  
	  
	
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/movie_small.gif" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	        <!--        <p><a href="https://arxiv.org/abs/1605.07651"> -->
	<papertitle>Bringing Alive Blurred Moments</papertitle></a><br><strong>Kuldeep Purohit</strong>*, Anshul Shah, and A.N. Rajagopalan<br>
                 			                  <strong>Accepted for Oral Presentation at IEEE Conference on Computer Vision and Pattern Recognition (CVPR 2019), Long Beach, CA, USA, June 2019</strong> <br>
<a href="https://arxiv.org/abs/1804.02913.pdf">ArXiv version</a> /
<a href="./files/CVPR2019_supplementary.pdf">Supplementary</a> /
<a href="https://github.com/anshulbshah/Blurred-Image-to-Video">Project Page</a>			
	 <!-- 	  <a href="https://github.com/moinnabi/SelfPacedDeepLearning">code</a> /
                  <a href="http://dblp.uni-trier.de/rec/bib2/journals/corr/SanginetoNCS16.bib">bibtex</a> -->
                </p><p></p>
                <p>Designed a deep convolutional architecture to extract a sharp video from a motion blurred
image. The first stage involves unsupervised training of a novel spatiotemporal network
for motion extraction from short video sequences. The above network is utilized for guided
training of a CNN which extracts the same motion embedding from a single blurred image.
The above networks are finally linked with our efficient deblurring network to generate the
sharp video. Our framework delivers state-of-the-art accuracy in single image deblurring
and video extraction while being faster and more compact.
</br></br> 
<!-- <small>*Authors contributed equally</small>-->

		</p><p></p>
                <p></p>
              </td>
            </tr>



          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/thumb_400.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	        <!--        <p><a href="https://arxiv.org/abs/1605.07651"> -->
	<papertitle>Mixed-Dense Connection Networks for Image and Video Super-Resolution</papertitle></a><br><strong>Kuldeep Purohit</strong>*, Srimanta Mandal, and A.N. Rajagopalan<br>
			                  <strong>Elsevier Neurocomputing (Special Issue on Deep Learning for Image Super-Resolution) 2019</strong> <br>
                <a href="https://www.sciencedirect.com/science/article/pii/S0925231219314614?via%3Dihub">Paper Link</a>
	 <!--  	  <a href="https://github.com/moinnabi/SelfPacedDeepLearning">code</a> /
                  <a href="http://dblp.uni-trier.de/rec/bib2/journals/corr/SanginetoNCS16.bib">bibtex</a> -->
                </p><p></p>
                <p>Proposed a deep architecture for image and video super-resolution, which is built using efficient convolutional units we refer to as mixed-dense connection blocks, whose design combines the strengths of both residual and dense connection strategies, while overcoming their limitations. We enable efficient super-resolution for higher scale-factors through our scale-recurrent framework which reutilizes the filters learnt for lower scale factors recursively for higher factors. We analyze the effects of loss configurations and demonstrate their utility in enhancing complementary image qualities. The proposed networks lead to state-of-the-art results on image and video super-resolution benchmarks.  
</br></br> 
<!-- <small>*Authors contributed equally</small>-->

		</p><p></p>
                <p></p>
              </td>
            </tr>








<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./files/Depth-map.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Planar Geometry and Latest Scene Recovery from a Single Motion Blurred Image</papertitle></a><br><strong>Kuldeep Purohit</strong>, Subeesh Vasu, M. Purnachandra Rao, and A.N. Rajagopalan<br>
                  <strong>Under Review</strong>* <br>
                  <a href="https://arxiv.org/abs/1904.03710">ArXiv Version</a> 
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a> -->
                </p><p></p>
                <p>Existing works on motion deblurring either ignore the effects of depth-dependent blur or work with the assumption of a 
			multi-layered scene wherein each layer is modeled in the form of fronto-parallel plane. In this work, we consider
			the case of 3D scenes with piecewise planar structure i.e., a scene that can be modeled as a combination of multiple 
			planes with arbitrary orientations. We first propose an approach for estimation of normal of a planar scene from a 
			single motion blurred observation. We then develop an algorithm for automatic recovery of a number of planes, the 
			parameters corresponding to each plane, and camera motion from a single motion blurred image of a multiplanar 3D scene.
			Finally, we propose a first-of-its-kind approach to recover the planar geometry and latent image of the scene by 
			adopting an alternating minimization framework built on our findings. Experiments on synthetic and real data reveal 
			that our proposed method achieves state-of-the-art results.  
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>



<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/UW_Dehazing.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Multi-level Weighted Enhancment for Underwater Image Dehazing</papertitle></a><br><strong>Kuldeep Purohit</strong>, Srimanta Mandal and A.N. Rajagopalan<br>
                  <strong>Journal of the Optical Society of America A (JOSA-A) </strong>* <br>
                  <a href="https://www.osapublishing.org/josaa/abstract.cfm?uri=josaa-36-6-1098">Paper Link</a> 
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a> -->
                </p><p></p>
                <p>Attenuation and scattering of light are responsible for haziness in images of underwater
scenes. We propose an approach to reduce this effect, based on the underlying principle is that enhancement at different
levels of detail can undo the degradation caused by underwater haze. The depth information is
captured implicitly while going through different levels of details due to depth-variant nature of
haze. Hence, we judiciously assign weights to different levels of image details and reveal that
their linear combination along with the coarsest information can successfully restore the image.
Results demonstrate the efficacy of our approach as compared to state-of-the-art underwater
dehazing methods. 
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./files/ICCV_thumbprint.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>EFFICIENT MOTION DEBLURRING WITH FEATURE TRANSFORMATION AND SPATIAL ATTENTION</papertitle></a><br><strong>Kuldeep Purohit</strong> and A.N. Rajagopalan<br>
                  <strong>Accepted at the IEEE International Conference on Image Processing (ICIP) 2019</strong>* <br>
                  <a href="https://arxiv.org/abs/1903.11394">ArXiv Version</a> 
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a> -->
                </p><p></p>
                <p> A preliminary version of our work "Spatially-Adaptive Residual Networks for Efficient Image and Video Deblurring"</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/PD_plot.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	        <!--        <p><a href="https://arxiv.org/abs/1605.07651"> -->
	<papertitle>Scale-Recurrent Multi-residual Dense Network for Image Super-Resolution</papertitle></a><br><strong>Kuldeep Purohit</strong>*, Srimanta Mandal, and A.N. Rajagopalan<br>
			                  <strong>PIRM Workshop and Challenge, Eurpean Conference on Computer Vision Workshops (ECCVW 2018), Munich, Germany, September 2018</strong> <br>

 <a href="http://openaccess.thecvf.com/content_ECCVW_2018/papers/11133/Purohit_Scale-Recurrent_Multi-Residual_Dense_Network_for_Image_Super-Resolution_ECCVW_2018_paper.pdf">Paper</a> /
 <a href="./files/Poster_ECCV2018.pdf">Poster</a> 

          <!--       <a href="https://arxiv.org/abs/1804.02913.pdf">ArXiv Pre-print</a>
	  	  <a href="https://github.com/moinnabi/SelfPacedDeepLearning">code</a> /
                  <a href="http://dblp.uni-trier.de/rec/bib2/journals/corr/SanginetoNCS16.bib">bibtex</a> -->
                </p><p></p>
                <p>A preliminary version of our Neurocomputing work, presented at the European Conference on Computer Vision (ECCV) - Perceptual Image Restoration and Manipulation (PIRM) Workshop 2018. Our team REC-SR was a <strong>finalist in all three regions of the Super Resolution Challenge</strong> (https://www.pirm2018.org/PIRM-SR.html).
</br></br> 
<!-- <small>*Authors contributed equally</small>-->

		</p><p></p>
                <p></p>
              </td>
            </tr>




<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/blur_detection.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	   <!--              <p><a href="https://arxiv.org/abs/1610.00307"> -->
	<papertitle>Learning based Blur Detection and Segmentation</papertitle></a><br>
		 <strong>Kuldeep Purohit</strong>, Anshul B. Shah, and A.N. Rajagopalan<br>
                  <strong>IEEE International Conference on Image Processing (ICIP 2018), Athens, Greece, October 2018  </strong> <br>
                  <a href="https://ieeexplore.ieee.org/document/8451765">Paper Link</a> /
                  <a href="./files/ICIP2018_supplementary.pdf">Supplementary</a> /
                  <a href="./files/POSTER_ICIP2018.pdf">Poster</a> 
        <!--           <em>arXiv:1610.00307</em>, 2016 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/1610.00307v1.pdf">PDF</a> /
                  <a href="#">bibtex</a> -->
                </p><p></p>
                <p>We present a robust two-level architecture for blur-based segmentation of a single image. First network is a fully convolutional encoder-decoder for estimating a semantically meaningful blur map from the full-resolution blurred image. Second network is a CNN-based classifier for obtaining local (patch-level) blur-probabilities. Fusion of the two network outputs enables accurate blur-segmentation using Graph-cut optimization over the obtained probabilities. We also show its applications in blur magnification and matting.
                </p><p></p>
                <p></p>
              </td>
            </tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/icvgip2019.png" alt="PontTuset" width="150" style="border-style: none">
	              </td><td width="75%" valign="top">
	   <!--              <p><a href="https://arxiv.org/abs/1610.00307"> -->
	<papertitle>Color Image Super Resolution in Real Noise</papertitle></a><br>
		 Srimanta Mandal, <strong>Kuldeep Purohit</strong>, and A.N. Rajagopalan<br>
                  <strong>Accepted for Oral Presentation at ACM Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP 2018), IIIT Hyderabad, India, december 2018 </strong> <br>
                   <a href="./files/ICVGIP2018_Cam.pdf">Accepted Version</a> 
         <!--         <em>arXiv:1610.00307</em>, 2016 &nbsp; <br>
                  <a href="https://arxiv.org/pdf/1610.00307v1.pdf">PDF</a> /
                  <a href="#">bibtex</a> -->
                </p><p></p>
                <p> Proposed an approach to super-resolve noisy color images by considering the color channels
jointly. Implicit low-rank structure of visual data is enforced via nuclear norm minimization
in association with color channel-dependent weights, which are added as a regularization
term to the cost function. Additionally, multi-scale details of the image are added to the
model through another regularization term that involves projection onto PCA basis, which
is constructed using similar patches extracted across different scales of the input image. <strong> Selected for the Best Paper Award (Runner Up) </strong>*: https://cvit.iiit.ac.in/icvgip18/bestpaperaward.php.
                </p><p></p>
                <p></p>
              </td>
            </tr>


<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/ICVGIP.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Mosaicing Deep Underwater Imagery</papertitle></a><br><strong>Kuldeep Purohit</strong>,Subeesh Vasu, A.N. Rajagopalan, V Bala Naga Jyothi, and Ramesh Raju<br>
                  <strong>ACM Indian Conference on Computer Vision, Graphics and Image Processing (ICVGIP 2016), IIT Guwahati, India, december 2016 </strong> &nbsp; <br>
                  <a href="http://www.ee.iitm.ac.in/~ee13d050/pdf/2016_Kuldeep_Deep_icvgip.pdf">main paper</a> /
		<a href="https://drive.google.com/file/d/1COgdD_3fKe2Fq773SXR-k66RI972spWH/view">Supplementary</a> /
		<a href="./files/POSTER_ICVGIP2016.pdf">Poster</a> 
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a>  -->
                </p><p></p>
                <p>This work deals with the problem of mosaicing deep underwater images (captured by Remotely Operated Vehicles), which suffer from haze, color-cast, and non-uniform illumination. We propose a framework that restores these images in accordance with a suitably derived degradation model. Furthermore, our scheme harnesses the scene-depth information present in the haze for non-rigid registration of the images before blending to construct a mosaic that is free from artifacts such as local blurring, ghosting, and visible seams.
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>

<!--Next paper should be listed in bottom -->
	              <td width="25%"><img src="./img/splicing.png" alt="PontTuset" width="170" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Splicing Localization in Motion Blurred 3D scenes</papertitle></a><br><strong>Kuldeep Purohit</strong> and A.N. Rajagopalan<br>
                  <strong>IEEE International Conference on Image Processing (ICIP 2016), Phoenix, Arizona, September 2016</strong> &nbsp; <br>
                  <a href="https://ieeexplore.ieee.org/abstract/document/7533095">Paper</a> /				
                  <a href="./files/ICIP2016_supplementary.pdf">Supplementary</a> /
                  <a href="./files/POSTER_ICIP2016.pdf">Poster</a> 
      <!--            <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a> -->
                </p><p></p>
                <p> This work proposes an efficient algorithm for depth based segmentation using spatially-distributed blur-kernels present in a single motion-blurred image of a 3D scene. The segmentation is then further utilized to estimate global camera motion from a single blurred image of a 3D scene. Finally, local blur profiles are compared with the global motion model to highlight inconsistencies and detect spliced regions. 
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Co-authored Workshop Proceedings</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody><tr>

	              <td width="25%"><img src="./files/NTIRE2019.jpeg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>NTIRE 2019 Challenge on Image Colorization: Report</papertitle></a><br>
Shuhang Gu, Radu Timofte, Richard Zhang, Maitreya Suin, <strong>Kuldeep Purohit</strong> , A. N. Rajagopalan, Athi Narayanan S., Jameer Babu Pinjari, Zhiwei Xiong, Zhan Shi, Chang Chen, Dong Liu, Manoj Sharma, Megh Makwana, Anuj Badhwar, Ajay Pratap Singh, Avinash Upadhyay, Akkshita Trivedi, Anil Saini, Santanu Chaudhury, Prasen Kumar Sharma, Priyankar Jain, Arijit Sur, Gokhan Ozbulak <br>
                  <strong>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop</strong> <br>
                  <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/NTIRE/Gu_NTIRE_2019_Challenge_on_Image_Colorization_Report_CVPRW_2019_paper.pdf">PDF (Openacess)</a> 
          <!--        <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a> -->
                </p><p></p>
                <p> Report describing various solutions to the Challenge on Image Colorization, NTIRE 2019</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>		   
			   
		    
	              <td width="25%"><img src="./files/NTIRE2019.jpeg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>NTIRE 2019 Challenge on Video Deblurring: Methods and Results</papertitle></a><br>
Nah, Seungjun and Timofte, Radu and Baik, Sungyong and Hong, Seokil and Moon, Gyeongsik and Son, Sanghyun and Lee, Kyoung Mu and Wang, Xintao and Chan, Kelvin C.K. and Yu, Ke and Dong, Chao and Loy, Chen Change and Fan, Yuchen and Yu, Jiahui and Liu, Ding and Huang, Thomas S. and Sim, Hyeonjun and Kim, Munchurl and Park, Dongwon and Kim, Jisoo and Chun, Se Young and Haris, Muhammad and Shakhnarovich, Greg and Ukita, Norimichi and Zamir, Syed Waqas and Arora, Aditya and Khan, Salman and Khan, Fahad Shahbaz and Shao, Ling and Gupta, Rahul Kumar and Chudasama, Vishal and Patel, Heena and Upla, Kishor and Fan, Hongfei and Li, Guo and Zhang, Yumei and Li, Xiang and Zhang, Wenjie and He, Qingwen and <strong>Purohit, Kuldeep</strong> and Rajagopalan, A. N. and Kim, Jeonghun and Tofighi, Mohammad and Guo, Tiantong and Monga, Vishal <br>
                  <strong>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop</strong> <br>
                  <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/NTIRE/Nah_NTIRE_2019_Challenge_on_Video_Deblurring_Methods_and_Results_CVPRW_2019_paper.pdf">PDF (Openacess)</a> 
          <!--        <a href="https://arxiv.org/abs/1903.11394">ArXiv Version</a> 
                  <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a> -->
                </p><p></p>
                <p> Report describing various solutions to the Challenge on Video Deblurring, NTIRE 2019</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>

			   
			   
			   
			   <td width="25%"><img src="./files/NTIRE2019.jpeg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>NTIRE 2019 Challenge on Video Super-Resolution: Methods and Results</papertitle></a><br>
Nah, Seungjun and Timofte, Radu and Gu, Shuhang and Baik, Sungyong and Hong, Seokil and Moon, Gyeongsik and Son, Sanghyun and Lee, Kyoung Mu and Wang, Xintao and Chan, Kelvin C.K. and Yu, Ke and Dong, Chao and Loy, Chen Change and Fan, Yuchen and Yu, Jiahui and Liu, Ding and Huang, Thomas S. and Liu, Xiao and Li, Chao and He, Dongliang and Ding, Yukang and Wen, Shilei and Porikli, Fatih and Kalarot, Ratheesh and Haris, Muhammad and Shakhnarovich, Greg and Ukita, Norimichi and Yi, Peng and Wang, Zhongyuan and Jiang, Kui and Jiang, Junjun and Ma, Jiayi and Dong, Hang and Zhang, Xinyi and Hu, Zhe and Kim, Kwanyoung and Kang, Dong Un and Chun, Se Young and <strong>Purohit, Kuldeep</strong> and Rajagopalan, A. N. and Tian, Yapeng  and Zhang, Yulun and Fu, Yun and Xu, Chenliang and Tekalp, A. Murat and Yilmaz, M. Akin and Korkmaz, Cansu and Sharma, Manoj and Makwana, Megh and Badhwar, Anuj and Singh, Ajay Pratap and Upadhyay, Avinash and Mukhopadhyay, Rudrabha and Shukla, Ankit and Khanna, Dheeraj and Mandal, A.S. and Chaudhury, Santanu and Miao, Si and Zhu, Yongxin and Huo, Xiao <br>
                  <strong>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop</strong> <br>
                  <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/NTIRE/Nah_NTIRE_2019_Challenge_on_Video_Super-Resolution_Methods_and_Results_CVPRW_2019_paper.pdf">PDF (Openacess)</a> 
          <!--        <a href="https://arxiv.org/abs/1903.11394">ArXiv Version</a> 
                  <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a> -->
                </p><p></p>
                <p> Report describing various solutions to the Challenge on Video Super-Resolution, NTIRE 2019</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>
		    
		    
		    
	              <td width="25%"><img src="./files/NTIRE2019.jpeg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>NTIRE 2019 Image Dehazing Challenge Report</papertitle></a><br>
Codruta O. Ancuti, Cosmin Ancuti, Radu Timofte, Luc Van Gool, Lei Zhang,Ming-Hsuan Yang, Tiantong Guo, Xuelu Li, Venkateswararao Cherukuri, Vishal Monga, Hao Jiang, Siyuan Yang, Yan Liu, Xiaochao Qu, Pengfei Wan, Dongwon Park, Se Young Chun, Ming Hong, Jinying Huang, Yizi Chen, Shuxin Chen, Bomin Wang, Pablo Navarrete Michelini, Hanwen Liu, Dan Zhu, Jing Liu, Sanchayan Santra, Ranjan Mondal , Bhabatosh Chanda, Peter Morales, Tzofi Klinghoffer, Le Manh Quan, Yong-Guk Kim, Xiao Liang, Runde Li, Jinshan Pan, Jinhui Tang, <strong>Kuldeep Purohit</strong> , Maitreya Suin, Raimondo Schettini, Simone Bianco, Flavio Piccoli, C. Cusano, Luigi Celona, Sunhee Hwang, Hyeran Byun, Subrahmanyam Murala, Akshay Dudhane, Harsh Aulakh, Zheng Tianxiang, Tao Zhang, Weining Qin, Runnan Zhou, Shanhu Wang, Jean-Philippe Tarel, Chuansheng Wang, Jiawei Wu <br>
                  <strong>IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshop</strong> <br>
                  <a href="http://openaccess.thecvf.com/content_CVPRW_2019/papers/NTIRE/Ancuti_NTIRE_2019_Image_Dehazing_Challenge_Report_CVPRW_2019_paper.pdf">PDF (Openacess)</a> 
          <!--        <a href="https://arxiv.org/abs/1903.11394">ArXiv Version</a> 
                  <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a> -->
                </p><p></p>
                <p> Report describing various solutions to the Challenge on Image Dehazing, NTIRE 2019</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>

          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
		<tbody>
		   <tr>
			   
		    


<!--Next paper should be listed in bottom 
	              <td width="25%"><img src="./img/BagOfPoselet.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Human Action Recognition in Still Images using Bag of Latent Poselets</papertitle>*</a><br><strong>M. Nabi</strong>, M. Rahmati<br>
                  <em>9th European Conference on Visual Media Production (CVMP)</em>, 2012 &nbsp;<br>
                  <a href="./files/CVMP2012_abstract.pdf">PDF</a> /
                  <a href="./files/CVMP2012.bib">bibtex</a>
                </p><p></p>
                <p>We represent human body poses in a single images by extracting the Poselet activation vectors on it, and recognize human activities in still images using the proposed bag of latent Poselets.
</br></br>
<small>*This work is based on my MS thesis at AUT.</small>
		</p><p></p>
                <p></p>
              </td>
            </tr>
-->

<!--SECTION -->




          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
                <heading>Other Projects</heading>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20">
            <tbody><tr>

		    
		    

	              <td width="25%"><img src="./files/mstaa5c45f01_online.jpg" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Enhancement and Photo-metric Stereo for SEM images</papertitle></a><br><strong>Kuldeep Purohit</strong>, Arun M, Mahesh Mohan, and A.N. Rajagopalan<br>
                  <strong>Project with KLA-Tencor, India</strong> &nbsp; <br> 
                  2017 &nbsp; <br> 
              <!--    <a href="http://www.ee.iitm.ac.in/~ee13d050/pdf/2016_Kuldeep_Deep_icvgip.pdf">main paper</a> /
		<a href="https://drive.google.com/file/d/1COgdD_3fKe2Fq773SXR-k66RI972spWH/view">supplementary</a> / -->
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a>  -->
                </p><p></p>
                <p> Addressed the blind reconstruction problem in scanning electron microscope (SEM) photometric stereo for complicated semiconductor patterns to be measured. 
			Developed a scheme using domain-specific priors on surface and sensor patterns in the optimization framework for robust estimation of the 3D surface structures.
			Also developed a user-centric detail enhancement scheme for improving visual quality of noisy SEM images.
			Proposed appraoch was validated through experiments on real data and was deployed commercially.
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>		    
		    
		    
	              <td width="25%"><img src="./files/A_robot_competing_in_the_Intelligent_Ground_Vehicle_competition.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Autonomous Bot for the Annual Intelligent Ground Vehicle Competition (IGVC)</papertitle></a> 
                  <strong>Team Abhiyaan, Centre for Innovation (CFI), IIT Madras, India</strong> &nbsp; <br> 
                  2017 &nbsp; <br> 
                 <a href="http://www.igvc.org/design/2017/7.pdf">Report</a> 
	<!-- 	<a href="https://drive.google.com/file/d/1COgdD_3fKe2Fq773SXR-k66RI972spWH/view">supplementary</a> / -->
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a>  -->
                </p><p></p> 
                <p> Our team designed a fully autonomous all-terrain ground vehicle. I specifically worked on the Computer Vision Module which involved algorithm development for 
	        real-time lane detection and obstacle segmentation task. The computer vision and path planning modules were integrated using ROS for autonomous navigation.
		The design was used in Vehicles that represented IIT Madras in the Intelligent Ground Vehicle Competition (IGVC) in 2017 and 2018.
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>

	              <td width="25%"><img src="./files/DRDO_thumbnail.png" alt="PontTuset" width="200" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Multiple Target Detection and Tracking in Wide Area Surveillance</papertitle></a><br><strong>Kuldeep Purohit</strong> and Arshad Jamal (Scientist E)<br> 
                  <strong>Project under Centre for Artificial Intelligence and Robotics, Defense Research and Development Organization, India</strong> &nbsp; <br> 
                  2012 &nbsp; <br> 
                 <a href="./files/DRDR_Report.pdf">Report</a> 
	<!-- 	<a href="https://drive.google.com/file/d/1COgdD_3fKe2Fq773SXR-k66RI972spWH/view">supplementary</a> / -->
           <!--       <a href="./files/UW2014_slides.pdf">slide</a> /
                  <a href="#">code</a>  -->
                </p><p></p> 
                <p> In this project, the problem of object detection and tracking in the challenging domain of wide area surveillance 
			has been tackled. This problem poses several challenges: large camera motion, strong parallax, large number of moving 
			objects, and small number of pixels on target, single channel data and low frame-rate of video. The method implemented
			here overcomes these challenges when tested on UAV videos.
</br></br>
<!-- <small>*This work was done while I was at UW.</small> -->
                </p><p></p>
                <p></p>
              </td>
            </tr>
		    
<!--Next paper should be listed in bottom -->
<!--

	              <td width="25%"><img src="./img/PhDthesis.jpg" alt="PontTuset" width="160" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="http://arxiv.org/abs/1512.07314">
	<papertitle>Mid-level Representation for Visual Recognition</papertitle></a><br>
                  <strong>Moin Nabi</strong><br>
                  <em>Ph.D. Dissertation </em>, 2015 &nbsp; <br>
                  <a href="./files/PhD_thesis.pdf">PDF</a> /
                  <a href="./files/thesis_slide.pdf">slides</a> /
                  <a href="https://www.youtube.com/watch?v=6IY-0swKaiM">talk</a> /
                </p><p></p>
                <p>This thesis targets employing mid-level representations for different high-level visual
recognition tasks, both in image and video understanding.
                </p><p></p>
                <p></p>
              </td>
            </tr>

-->

<!--Next paper should be listed in bottom -->

<!--
	              <td width="25%"><img src="./img/stock.jpg" alt="PontTuset" width="160" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>Stock trend prediction using Twin Gaussian Process regression</papertitle></a><br>M. Mojaddady, <strong>M. Nabi</strong>, S. Khadivi<br>
                  <em>Technical Report</em>, 2011 &nbsp;<br>
                  <a href="./files/stock.pdf">PDF</a> /
                  <a href="./files/stock.bib">bibtex</a>
                </p><p></p>
                <p>
		</p><p></p>
                <p></p>
              </td>
            </tr>

-->

<!--Next paper should be listed in bottom -->

<!--

	              <td width="25%"><img src="./img/matlab3.jpg" alt="PontTuset" width="140" style="border-style: none">
	              </td><td width="75%" valign="top">
	                <p><a href="#">
	<papertitle>A Turorial on Digital Image Processing using MATLAB</papertitle></a><br><strong>M. Nabi</strong><br>
                  <em>National Digital Image Processing Workshop</em>, 2008 &nbsp;<br>
                  <a href="./files/tutorial.zip">PDF</a> /
		  <a href="./files/code.zip">code</a> /
		  <a href="./files/images.tar.gz">data</a>
                </p><p></p>
                <p>
		</p><p></p>
                <p></p>
              </td>
            </tr>




          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>

-->

<!--                <br>
                <p align="right"><font size="3">
			Erd&ouml;s = 3 (via two paths)
                  </font>
                <br>
-->


 <!-- 		<p align="right"><font size="2">
                  <a href="http://www.cs.berkeley.edu/~barron/">Thanks Jon Barron</a>
                  </font>
                </p>
              </td>
            </tr>
          </tbody></table>
          <script type="text/javascript">
            var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
                    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
                    
          </script><script src="./img/ga.js" type="text/javascript"></script> <script type="text/javascript">
            try {
                    var pageTracker = _gat._getTracker("UA-7580334-1");
                    pageTracker._trackPageview();
                    } catch(err) {}
                    
          </script>
        </td>
      </tr>
    </tbody></table>
 -->

	            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
            <tbody><tr>
              <td>
<div style="text-align: left;"><script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5fz2vvb1pjx&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>
              </td>
            </tr>
          </tbody></table>



	<!-- <script type="text/javascript" src="//ri.revolvermaps.com/0/0/1.js?i=834fq7qvtyr&amp;s=182&amp;m=0&amp;v=false&amp;r=false&amp;b=000000&amp;n=false&amp;c=ff0000"align="left" async="async"></script></div>
<script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5fz2vvb1pjx&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script> -->
</body></html>
